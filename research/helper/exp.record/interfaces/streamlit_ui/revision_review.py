"""
ä¿®è®¢å®¡æ ¸ç•Œé¢ - é«˜äº®æ˜¾ç¤ºä¿®æ”¹éƒ¨åˆ†ï¼Œå¼ºåˆ¶ç”¨æˆ·ç¡®è®¤
"""

import streamlit as st
import logging
from datetime import datetime
from typing import Dict, Any, List
import difflib
from pathlib import Path

from config.settings import Settings
from storage.experiment_store import ExperimentStore
from storage.template_manager import TemplateManager
from utils.diff_utils import highlight_modifications, generate_side_by_side_diff


def render_revision_review():
    """æ¸²æŸ“ä¿®è®¢å®¡æ ¸æ ‡ç­¾é¡µ"""
    st.title("âœ… ä¿®è®¢å®¡æ ¸")
    st.markdown("---")
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "review_mode" not in st.session_state:
        st.session_state.review_mode = "list"
    if "reviewing_experiment" not in st.session_state:
        st.session_state.reviewing_experiment = None
    
    # ä¾§è¾¹æ æ“ä½œ
    with st.sidebar:
        st.markdown("### ğŸ” å®¡æ ¸æ“ä½œ")
        
        if st.button("ğŸ“‹ å¾…å®¡æ ¸åˆ—è¡¨", key="review_list"):
            st.session_state.review_mode = "list"
            st.session_state.reviewing_experiment = None
            st.rerun()
        
        if st.button("ğŸ“Š å®¡æ ¸ç»Ÿè®¡", key="review_stats"):
            st.session_state.review_mode = "stats"
            st.session_state.reviewing_experiment = None
            st.rerun()
        
        # ç­›é€‰é€‰é¡¹
        st.markdown("---")
        st.markdown("### ğŸ“‚ ç­›é€‰é€‰é¡¹")
        
        status_filter = st.selectbox(
            "çŠ¶æ€ç­›é€‰:",
            ["å…¨éƒ¨", "å¾…å®¡æ ¸", "å·²é€šè¿‡", "éœ€ä¿®æ”¹"],
            key="status_filter"
        )
        
        date_filter = st.selectbox(
            "æ—¶é—´ç­›é€‰:",
            ["å…¨éƒ¨", "ä»Šå¤©", "æœ¬å‘¨", "æœ¬æœˆ"],
            key="date_filter"
        )
    
    # æ¸²æŸ“ä¸»è¦å†…å®¹
    if st.session_state.review_mode == "list":
        _render_review_list(status_filter, date_filter)
    elif st.session_state.review_mode == "detail":
        _render_review_detail()
    elif st.session_state.review_mode == "stats":
        _render_review_statistics()


def _render_review_list(status_filter: str, date_filter: str):
    """æ¸²æŸ“å®¡æ ¸åˆ—è¡¨"""
    st.markdown("## ğŸ“‹ å¾…å®¡æ ¸å®éªŒåˆ—è¡¨")
    
    try:
        experiment_store = ExperimentStore()
        experiments = experiment_store.list_experiments(limit=50)
        
        # åº”ç”¨ç­›é€‰
        filtered_experiments = _apply_filters(experiments, status_filter, date_filter)
        
        if not filtered_experiments:
            st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å®éªŒè®°å½•")
            return
        
        # æ˜¾ç¤ºå®éªŒå¡ç‰‡
        for exp in filtered_experiments:
            with st.expander(f"ğŸ§ª {exp['title']} ({exp['created_at'][:10]})"):
                col1, col2, col3 = st.columns([3, 1, 1])
                
                with col1:
                    # è·å–å®Œæ•´çš„å®éªŒæ•°æ®ä»¥æ˜¾ç¤ºéªŒè¯çŠ¶æ€
                    full_exp = experiment_store.get_experiment(exp['id'])
                    if full_exp:
                        validation_result = full_exp.get('validation_result', {})
                        confidence = validation_result.get('confidence', 0)
                        is_valid = validation_result.get('is_valid', False)
                        
                        # çŠ¶æ€æŒ‡ç¤º
                        if is_valid and confidence >= 0.8:
                            status_color = "ğŸŸ¢"
                            status_text = "éªŒè¯é€šè¿‡"
                        elif is_valid and confidence >= 0.6:
                            status_color = "ğŸŸ¡"
                            status_text = "åŸºæœ¬é€šè¿‡"
                        else:
                            status_color = "ğŸ”´"
                            status_text = "éœ€å®¡æ ¸"
                        
                        st.markdown(f"**çŠ¶æ€**: {status_color} {status_text}")
                        st.markdown(f"**ç½®ä¿¡åº¦**: {confidence:.2f}")
                        st.markdown(f"**æ¨¡æ¿**: {exp['template_id']}")
                        
                        # æ˜¾ç¤ºé—®é¢˜æ‘˜è¦
                        if validation_result.get('issues'):
                            st.error("é—®é¢˜:")
                            for issue in validation_result['issues'][:2]:  # åªæ˜¾ç¤ºå‰2ä¸ª
                                st.write(f"- {issue}")
                
                with col2:
                    if st.button("ğŸ‘ï¸ è¯¦æƒ…", key=f"detail_{exp['id']}"):
                        st.session_state.reviewing_experiment = exp['id']
                        st.session_state.review_mode = "detail"
                        st.rerun()
                
                with col3:
                    if st.button("âœ… æ‰¹å‡†", key=f"approve_{exp['id']}"):
                        _approve_experiment(exp['id'])
                    
                    if st.button("âŒ æ‹’ç»", key=f"reject_{exp['id']}"):
                        _reject_experiment(exp['id'])
    
    except Exception as e:
        st.error(f"åŠ è½½å®éªŒåˆ—è¡¨å¤±è´¥: {e}")


def _render_review_detail():
    """æ¸²æŸ“è¯¦ç»†å®¡æ ¸ç•Œé¢"""
    if not st.session_state.reviewing_experiment:
        st.session_state.review_mode = "list"
        st.rerun()
    
    try:
        experiment_store = ExperimentStore()
        template_manager = TemplateManager()
        
        experiment = experiment_store.get_experiment(st.session_state.reviewing_experiment)
        if not experiment:
            st.error("å®éªŒè®°å½•ä¸å­˜åœ¨")
            return
        
        template = template_manager.get_template(experiment['template_id'])
        if not template:
            st.error("å…³è”æ¨¡æ¿ä¸å­˜åœ¨")
            return
        
        st.markdown(f"## ğŸ” è¯¦ç»†å®¡æ ¸: {experiment['title']}")
        
        # åŸºæœ¬ä¿¡æ¯
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æ¨¡æ¿", template['name'])
        
        with col2:
            validation_result = experiment.get('validation_result', {})
            st.metric("ç½®ä¿¡åº¦", f"{validation_result.get('confidence', 0):.2f}")
        
        with col3:
            is_valid = validation_result.get('is_valid', False)
            st.metric("éªŒè¯çŠ¶æ€", "é€šè¿‡" if is_valid else "å¤±è´¥")
        
        with col4:
            st.metric("åˆ›å»ºæ—¶é—´", experiment['created_at'][:10])
        
        # ç”¨æˆ·ä¿®æ”¹æè¿°
        st.markdown("### ğŸ“ ç”¨æˆ·ä¿®æ”¹éœ€æ±‚")
        st.text_area("", experiment['user_modifications'], height=100, disabled=True)
        
        # éªŒè¯ç»“æœè¯¦æƒ…
        validation_result = experiment.get('validation_result', {})
        _render_validation_details(validation_result)
        
        # å·®å¼‚å¯¹æ¯”
        st.markdown("### ğŸ” ä¿®è®¢å†…å®¹å¯¹æ¯”")
        
        tab1, tab2, tab3 = st.tabs(["å¹¶æ’å¯¹æ¯”", "é«˜äº®ä¿®æ”¹", "å®Œæ•´ç‰ˆæœ¬"])
        
        with tab1:
            # å¹¶æ’å¯¹æ¯”
            side_by_side = generate_side_by_side_diff(
                experiment['original_template'],
                experiment['revised_content']
            )
            st.markdown(side_by_side, unsafe_allow_html=True)
        
        with tab2:
            # é«˜äº®ä¿®æ”¹
            highlighted = highlight_modifications(
                experiment['original_template'],
                experiment['revised_content']
            )
            st.markdown(highlighted, unsafe_allow_html=True)
        
        with tab3:
            # å®Œæ•´ä¿®è®¢ç‰ˆæœ¬
            st.text_area(
                "ä¿®è®¢åå®Œæ•´ç‰ˆæœ¬:",
                experiment['revised_content'],
                height=400,
                disabled=True
            )
        
        # ä¿®è®¢å†å²
        revision_history = experiment_store.get_revision_history(experiment['id'])
        if revision_history:
            st.markdown("### ğŸ“š ä¿®è®¢å†å²")
            
            for revision in revision_history:
                with st.expander(f"ä¿®è®¢ {revision['revision_number']}: {revision['change_description']}"):
                    st.write(f"æ—¶é—´: {revision['created_at']}")
                    st.write(f"ç±»å‹: {revision['change_type']}")
                    
                    if revision.get('user_prompt'):
                        st.text_area("ç”¨æˆ·æç¤º:", revision['user_prompt'], height=80, disabled=True)
        
        # å®¡æ ¸æ“ä½œ
        st.markdown("---")
        st.markdown("### ğŸ¯ å®¡æ ¸å†³å®š")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("è¿”å›åˆ—è¡¨"):
                st.session_state.review_mode = "list"
                st.session_state.reviewing_experiment = None
                st.rerun()
        
        with col2:
            if st.button("âœ… æ‰¹å‡†", type="primary"):
                _approve_experiment(experiment['id'])
        
        with col3:
            if st.button("âŒ æ‹’ç»"):
                _reject_experiment(experiment['id'])
        
        with col4:
            if st.button("ğŸ”„ è¦æ±‚é‡å®¡"):
                _request_rereview(experiment['id'])
    
    except Exception as e:
        st.error(f"åŠ è½½å®éªŒè¯¦æƒ…å¤±è´¥: {e}")
        logging.error(f"Review detail error: {e}")


def _render_validation_details(validation_result: Dict[str, Any]):
    """æ¸²æŸ“éªŒè¯ç»“æœè¯¦æƒ…"""
    if not validation_result:
        return
    
    # é—®é¢˜åˆ—è¡¨
    if validation_result.get('issues'):
        st.error("### âŒ éªŒè¯é—®é¢˜")
        for i, issue in enumerate(validation_result['issues'], 1):
            st.write(f"{i}. {issue}")
    
    # è­¦å‘Šåˆ—è¡¨
    if validation_result.get('warnings'):
        st.warning("### âš ï¸ è­¦å‘Šä¿¡æ¯")
        for i, warning in enumerate(validation_result['warnings'], 1):
            st.write(f"{i}. {warning}")
    
    # ä¿å®ˆå»ºè®®
    if validation_result.get('conservative_suggestions'):
        st.info("### ğŸ’¡ ä¿å®ˆæ¨¡å¼å»ºè®®")
        for i, suggestion in enumerate(validation_result['conservative_suggestions'], 1):
            st.write(f"{i}. {suggestion}")
    
    # ä¿®æ”¹è¯¦æƒ…
    modifications = validation_result.get('modifications', [])
    if modifications:
        st.success("### âœ… æœ‰æ•ˆä¿®æ”¹")
        for i, mod in enumerate(modifications, 1):
            st.write(f"{i}. **{mod.get('section', 'æœªçŸ¥ç« èŠ‚')}**: {mod.get('justification', 'æ— ä¾æ®')}")
            st.write(f"   ç½®ä¿¡åº¦: {mod.get('confidence', 0):.2f}")


def _render_review_statistics():
    """æ¸²æŸ“å®¡æ ¸ç»Ÿè®¡"""
    st.markdown("## ğŸ“Š å®¡æ ¸ç»Ÿè®¡")
    
    try:
        experiment_store = ExperimentStore()
        experiments = experiment_store.list_experiments(limit=1000)  # è·å–æ›´å¤šæ•°æ®ç”¨äºç»Ÿè®¡
        
        if not experiments:
            st.info("æš‚æ— æ•°æ®")
            return
        
        # ç»Ÿè®¡åˆ†æ
        stats = _calculate_review_stats(experiments)
        
        # æ€»ä½“ç»Ÿè®¡
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æ€»å®éªŒæ•°", stats['total_experiments'])
        
        with col2:
            st.metric("éªŒè¯é€šè¿‡", stats['passed_validations'])
        
        with col3:
            st.metric("éœ€å®¡æ ¸", stats['need_review'])
        
        with col4:
            st.metric("å¹³å‡ç½®ä¿¡åº¦", f"{stats['avg_confidence']:.2f}")
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        st.markdown("### ğŸ“ˆ ç½®ä¿¡åº¦åˆ†å¸ƒ")
        confidence_data = stats['confidence_distribution']
        
        if confidence_data:
            import pandas as pd
            df = pd.DataFrame(list(confidence_data.items()), columns=['ç½®ä¿¡åº¦åŒºé—´', 'æ•°é‡'])
            st.bar_chart(df.set_index('ç½®ä¿¡åº¦åŒºé—´'))
        
        # é—®é¢˜ç±»å‹ç»Ÿè®¡
        st.markdown("### ğŸš¨ å¸¸è§é—®é¢˜ç±»å‹")
        issue_types = stats['issue_types']
        
        if issue_types:
            for issue_type, count in issue_types.most_common(5):
                st.write(f"- {issue_type}: {count} æ¬¡")
        
        # æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡
        st.markdown("### ğŸ“‹ æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡")
        template_usage = stats['template_usage']
        
        if template_usage:
            for template_id, count in template_usage.most_common(10):
                st.write(f"- {template_id}: {count} æ¬¡ä½¿ç”¨")
    
    except Exception as e:
        st.error(f"åŠ è½½ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")


def _apply_filters(experiments: List[Dict[str, Any]], status_filter: str, date_filter: str) -> List[Dict[str, Any]]:
    """åº”ç”¨ç­›é€‰æ¡ä»¶"""
    filtered = experiments.copy()
    
    # çŠ¶æ€ç­›é€‰
    if status_filter != "å…¨éƒ¨":
        experiment_store = ExperimentStore()
        temp_filtered = []
        
        for exp in filtered:
            full_exp = experiment_store.get_experiment(exp['id'])
            if full_exp:
                validation_result = full_exp.get('validation_result', {})
                confidence = validation_result.get('confidence', 0)
                is_valid = validation_result.get('is_valid', False)
                
                if status_filter == "å¾…å®¡æ ¸" and not is_valid:
                    temp_filtered.append(exp)
                elif status_filter == "å·²é€šè¿‡" and is_valid and confidence >= 0.8:
                    temp_filtered.append(exp)
                elif status_filter == "éœ€ä¿®æ”¹" and is_valid and confidence < 0.8:
                    temp_filtered.append(exp)
        
        filtered = temp_filtered
    
    # æ—¶é—´ç­›é€‰
    if date_filter != "å…¨éƒ¨":
        from datetime import datetime, timedelta
        
        now = datetime.now()
        if date_filter == "ä»Šå¤©":
            cutoff = now - timedelta(days=1)
        elif date_filter == "æœ¬å‘¨":
            cutoff = now - timedelta(days=7)
        elif date_filter == "æœ¬æœˆ":
            cutoff = now - timedelta(days=30)
        else:
            cutoff = None
        
        if cutoff:
            temp_filtered = []
            for exp in filtered:
                exp_date = datetime.fromisoformat(exp['created_at'])
                if exp_date >= cutoff:
                    temp_filtered.append(exp)
            filtered = temp_filtered
    
    return filtered


def _calculate_review_stats(experiments: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è®¡ç®—å®¡æ ¸ç»Ÿè®¡æ•°æ®"""
    from collections import Counter
    
    experiment_store = ExperimentStore()
    
    stats = {
        'total_experiments': len(experiments),
        'passed_validations': 0,
        'need_review': 0,
        'avg_confidence': 0.0,
        'confidence_distribution': Counter(),
        'issue_types': Counter(),
        'template_usage': Counter()
    }
    
    confidences = []
    
    for exp in experiments:
        full_exp = experiment_store.get_experiment(exp['id'])
        if not full_exp:
            continue
        
        validation_result = full_exp.get('validation_result', {})
        confidence = validation_result.get('confidence', 0)
        is_valid = validation_result.get('is_valid', False)
        
        # ç»Ÿè®¡éªŒè¯çŠ¶æ€
        if is_valid and confidence >= 0.8:
            stats['passed_validations'] += 1
        elif not is_valid or confidence < 0.6:
            stats['need_review'] += 1
        
        # ç½®ä¿¡åº¦ç»Ÿè®¡
        confidences.append(confidence)
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        if confidence >= 0.9:
            stats['confidence_distribution']['0.9-1.0'] += 1
        elif confidence >= 0.8:
            stats['confidence_distribution']['0.8-0.9'] += 1
        elif confidence >= 0.6:
            stats['confidence_distribution']['0.6-0.8'] += 1
        else:
            stats['confidence_distribution']['0.0-0.6'] += 1
        
        # é—®é¢˜ç±»å‹ç»Ÿè®¡
        for issue in validation_result.get('issues', []):
            # ç®€å•çš„é—®é¢˜åˆ†ç±»
            if "ç« èŠ‚" in issue:
                stats['issue_types']['ç« èŠ‚é—®é¢˜'] += 1
            elif "ä¸å¯ä¿®æ”¹" in issue:
                stats['issue_types']['ä¸å¯ä¿®æ”¹ç« èŠ‚é—®é¢˜'] += 1
            elif "ä¾æ®" in issue:
                stats['issue_types']['ä¿®æ”¹ä¾æ®é—®é¢˜'] += 1
            else:
                stats['issue_types']['å…¶ä»–é—®é¢˜'] += 1
        
        # æ¨¡æ¿ä½¿ç”¨ç»Ÿè®¡
        stats['template_usage'][exp['template_id']] += 1
    
    # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
    if confidences:
        stats['avg_confidence'] = sum(confidences) / len(confidences)
    
    return stats


def _approve_experiment(experiment_id: str):
    """æ‰¹å‡†å®éªŒ"""
    try:
        experiment_store = ExperimentStore()
        
        # æ·»åŠ æ‰¹å‡†è®°å½•åˆ°ä¿®è®¢å†å²
        experiment_store.add_revision(experiment_id, {
            "change_type": "approval",
            "change_description": "å®éªŒè®°å½•å®¡æ ¸é€šè¿‡",
            "user_prompt": "ç®¡ç†å‘˜æ‰¹å‡†",
            "validation_result": {"approved": True}
        })
        
        st.success(f"âœ… å®éªŒ {experiment_id} å·²æ‰¹å‡†")
        st.rerun()
        
    except Exception as e:
        st.error(f"æ‰¹å‡†å®éªŒå¤±è´¥: {e}")
        logging.error(f"Approve experiment error: {e}")


def _reject_experiment(experiment_id: str):
    """æ‹’ç»å®éªŒ"""
    try:
        experiment_store = ExperimentStore()
        
        # æ·»åŠ æ‹’ç»è®°å½•åˆ°ä¿®è®¢å†å²
        experiment_store.add_revision(experiment_id, {
            "change_type": "rejection",
            "change_description": "å®éªŒè®°å½•å®¡æ ¸æœªé€šè¿‡",
            "user_prompt": "ç®¡ç†å‘˜æ‹’ç»",
            "validation_result": {"approved": False}
        })
        
        st.success(f"âŒ å®éªŒ {experiment_id} å·²æ‹’ç»")
        st.rerun()
        
    except Exception as e:
        st.error(f"æ‹’ç»å®éªŒå¤±è´¥: {e}")
        logging.error(f"Reject experiment error: {e}")


def _request_rereview(experiment_id: str):
    """è¦æ±‚é‡æ–°å®¡æ ¸"""
    try:
        experiment_store = ExperimentStore()
        
        # æ·»åŠ é‡å®¡è®°å½•åˆ°ä¿®è®¢å†å²
        experiment_store.add_revision(experiment_id, {
            "change_type": "rerequest",
            "change_description": "è¦æ±‚é‡æ–°å®¡æ ¸",
            "user_prompt": "ç®¡ç†å‘˜è¦æ±‚é‡å®¡",
            "validation_result": {"rereview_requested": True}
        })
        
        st.success(f"ğŸ”„ å·²è¦æ±‚é‡æ–°å®¡æ ¸å®éªŒ {experiment_id}")
        st.rerun()
        
    except Exception as e:
        st.error(f"è¦æ±‚é‡å®¡å¤±è´¥: {e}")
        logging.error(f"Request rereview error: {e}")